{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Challenge WS 2022/23\n",
    "\n",
    "#### Task:\n",
    "\n",
    "Your Task is to train a clickbait filter to classify clickbait articles by their headline.\n",
    "\n",
    "#### Dataset:\n",
    "\n",
    "The data consists of two files, a text file with clickbait headlines and one with headlines from news sources. The hold out dataset is organized the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the clickbait data and assigning class 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('clickbait_yes') as fyes:\n",
    "    linesyes = [line.rstrip() for line in fyes]\n",
    "\n",
    "dfyes = pd.DataFrame(linesyes, columns=['headlines'])\n",
    "\n",
    "dfyes['class']= 1\n",
    "dfyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions?\n",
    "\n",
    "[kuglerk@uni-trier.de](mailto:kuglerk@uni-trier.de?subject=ML%20Challenge%20NLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the no clickbait data and assigning class 0\n",
    "\n",
    "with open('clickbait_no') as fno:\n",
    "    linesno = [line.rstrip() for line in fno]\n",
    "\n",
    "dfno = pd.DataFrame(linesno, columns=['headlines'])\n",
    "\n",
    "dfno['class']= 0\n",
    "dfno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the test set data\n",
    "with open('clickbait_hold_X.csv') as ftest:\n",
    "    linesno = [line.rstrip() for line in ftest]\n",
    "\n",
    "X_test= pd.DataFrame(linesno, columns=['headlines'])\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a single data set with clickbait and non click bait datasets\n",
    "df = dfyes.append(dfno)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to preprocess and clean the dataframe remove punctuations, make lower case, remove stop words\n",
    "def preprocess(df, col, lang):\n",
    "    df[col] = df[col].str.replace(r'<[^<>]*>', '', regex=True)\n",
    "\n",
    "    df[col] = df[col].str.lower()\n",
    "    if lang=='en':\n",
    "        df[col] = df[col].str.replace(r\"n\\'t\", \" not\", regex=True)\n",
    "        df[col] = df[col].str.replace(r\"\\'t\", \" not\", regex=True)\n",
    "\n",
    "    df[col] = df[col].str.replace(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\/\\,])', r' \\1 ', regex=True)\n",
    "    df[col] = df[col].str.replace(r'[^\\w\\s\\?]', ' ', regex=True)\n",
    "    df[col] = df[col].str.replace(r'([\\;\\:\\|•«\\n])', ' ', regex=True)\n",
    "    return df\n",
    "\n",
    "#preprocessing of dataset \n",
    "preprocess(df, 'headlines', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing test data\n",
    "preprocess(X_test, 'headlines', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = ['class']).copy()\n",
    "y = df['class']\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from scipy import sparse\n",
    "\n",
    "#Create stopwords list\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "#TF-IDF Vectorization for train and test data\n",
    "tfidf = TfidfVectorizer(stop_words = stopwords_list,ngram_range = (1, 2))\n",
    "tfidf_headlines = tfidf.fit_transform(X['headlines'])\n",
    "tfidf_headlines_test = tfidf.transform(X_test['headlines'])\n",
    "\n",
    "X_ef = X.drop(columns='headlines')\n",
    "X_test_ef = X_test.drop(columns='headlines')\n",
    "\n",
    "X = sparse.hstack([X_ef, tfidf_headlines]).tocsr()\n",
    "X_test = sparse.hstack([X_test_ef, tfidf_headlines_test]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#splitting into train and validation datasets\n",
    "train_size = 0.8\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size = 0.8, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = y_valid.groupby(y_valid)\n",
    "g.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to the training data\n",
    "mn = MultinomialNB(alpha=.05)\n",
    "mn.fit(X_train, y_train)\n",
    "\n",
    "# use the model to make predictions on validation set\n",
    "validPredict = mn.predict(X_valid)\n",
    "\n",
    "validPredict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#f1 score for the model in validation set\n",
    "f1_score(y_valid, validPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make predictions on test set\n",
    "testPredict = mn.predict(X_test)\n",
    "\n",
    "testPredict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions for the test data\n",
    "testPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the prediction text file\n",
    "np.savetxt(r'predictions.txt', testPredict, fmt = '%d')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
